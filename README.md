# AIDiagnos
An intelligent multimodal AI assistant that listens to your voice, analyzes your uploaded image (such as a face or skin condition), and responds like a professional doctor â€” complete with text and spoken feedback.

This system combines speech recognition, image analysis, and text-to-speech synthesis using Groqâ€™s Whisper and LLaMA Vision models, creating a seamless voiceâ€“visionâ€“text interaction.

ğŸš€ Features

ğŸ™ï¸ Speech-to-Text: Converts spoken input into text using Groqâ€™s Whisper Large V3 model.

ğŸ©» Image Understanding: Analyzes medical-related images using LLaMA 3.2 Vision via Groq API.

ğŸ§  Doctor Simulation: Generates natural, human-like responses mimicking a doctorâ€™s advice.

ğŸ”Š Voice Response: Converts AIâ€™s medical feedback to speech using gTTS (Google Text-to-Speech).

ğŸ–¼ï¸ Interactive Gradio UI: Record voice, upload an image, and get spoken medical feedbackâ€”all in one app.

ğŸ§© Tech Stack

Groq API â€“ For LLaMA Vision and Whisper transcription.

Gradio â€“ For the user-friendly web interface.

SpeechRecognition â€“ For live voice capture.

pydub & gTTS â€“ For text-to-speech audio generation.

dotenv â€“ For secure environment variable management.

FFmpeg â€“ For audio conversion.

ğŸ“‚ Project Structure
â”œâ”€â”€ app.py                     # Main Gradio app script
â”œâ”€â”€ .env                        # Stores your Groq API key
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ final.mp3                   # Generated doctor voice output (runtime)
â””â”€â”€ README.md                   # Documentation

âš™ï¸ Setup Instructions
1. Clone the repository
git clone https://github.com/<your-username>/ai-doctor-vision-voice.git
cd ai-doctor-vision-voice

2. Create and activate a virtual environment
python -m venv venv
source venv/bin/activate       # For Linux/Mac
venv\Scripts\activate          # For Windows

3. Install dependencies
pip install -r requirements.txt

4. Create a .env file

Create a .env file in the root directory and add your Groq API key:

GROQ_API_KEY=your_groq_api_key_here


You can get your key from https://console.groq.com/keys

ğŸ§  How It Works

ğŸ¤ Voice Input:
The user records a voice query (e.g., â€œI have a rash on my cheek, what could it be?â€).

ğŸ”Š Speech Recognition (Groq Whisper):
The app converts your voice to text using the Whisper Large V3 model.

ğŸ–¼ï¸ Image Analysis (LLaMA Vision):
The uploaded image (like a selfie or skin photo) is analyzed by LLaMA 3.2 Vision for possible medical insights.

ğŸ©º AI Doctor Reasoning:
The system combines the text + image context, prompting the model to respond as a concise, empathetic doctor.

ğŸ”ˆ Text-to-Speech:
The AIâ€™s answer is synthesized into speech using gTTS, and played automatically.

ğŸ§© Example Usage

Launch the app:

python app.py


In the Gradio interface:

Record your voice input (e.g., â€œI have red spots on my face, what could it be?â€)

Upload a clear face image.

Click Submit.

The app will display:

Speech-to-Text output

Doctorâ€™s textual response

Spoken voice reply

ğŸ“¦ Example requirements.txt
groq
gradio
gtts
pydub
SpeechRecognition
python-dotenv
ffmpeg-python

ğŸ§‘â€âš•ï¸ System Prompt Logic

The AI is instructed to behave like a professional doctor (for learning and simulation only, not real medical use).
It responds in two short sentences, no markdown or list format, and no disclaimers like â€œAs an AI...â€.

system_prompt = """
You have to act as a professional doctor, i know you are not but this is for learning purpose.
What's in this image? Do you find anything wrong with it medically?
If you make a differential, suggest some remedies for them. Donâ€™t use numbers or bullet points.
Your answer should be concise (max 2 sentences) and human-like.
"""
